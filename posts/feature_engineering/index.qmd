---
title: "Feature Engineering"
author: "Ethan Tenison"
date: "4/26/2022"
categories: [feature engineering, analysis]
image: "image.jpg"
---

```{r setup, include = FALSE}
library(tidyverse)
library(scales)
library(janitor)
library(kableExtra)
library(reticulate)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
use_python("/Users/et/opt/miniconda3/bin/python")
#virtualenv_create("r-reticulate")
#virtualenv_install("r-reticulate", "pandas")
#virtualenv_install("r-reticulate", "scikit-learn")
#use_virtualenv("r-reticulate")
#py_install("pandas")
#pandas <- import("pandas")
#sklearn <- import("sklearn")
```

Improving ML models after they're already been built is fun and can lead to some immediate gratification. You might tweak the hyper parameters and cross validation to improve the generalizability of your model. You may switch out the algorithm itself. Lets say you move from using neural nets to gradient boosted trees to see if you can move the accuracy score even close to 99%. At a certain point though you won't be able to improve your model without over fitting. Often times, you need to take a step back and look at the raw data itself to see if new variables can be created from the old that lead to improvements. This is what's called feature engineering

**Feature Engineering** is the process of selecting, manipulating, and transforming your data into features, or variables, that are useful for machine learning. Whereas data cleaning is generally the process of subtracting irrelevant data, feature engineering is a process of addition, adding more relevant information to your data set.

# Simplification

Simplification is a major part of feature engineering. Combining two or more variables can increase the speed and accuracy of our model. Just to conceptualize what that would look like, I've pulled data from the [Ames Housing](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/) data set on Kaggle. It includes a variable for square footage and final sales prices.

```{r ames_housing}

ames_raw <- read_csv("posts/feature_engineering/house-prices-advanced-regression-techniques/train.csv")

ames <- ames_raw |> 
  clean_names() |> 
  select(id, gr_liv_area, sale_price) |>
  rename(sq_ft = gr_liv_area)

ames |> 
  head(10) |> 
  kbl() |> 
  kable_styling()
```

```{r}
#| label: plot_sale
options(scipen=10000)

theme_set(theme_classic())
ggplot(ames, aes(x=sale_price)) +
  geom_histogram(fill = "#2A9D8F") + 
  scale_x_continuous(labels=scales::dollar_format()) +
  labs(
    title = "Ames Housing Sale Price",
    x = "Sale Price",
    y = "Count"
  ) +
  theme(
     plot.title = element_text(hjust = 0.5)
  )
```

If we were in the neighborhood looking to buy a house, it would not be prudent to only consider the listing price. We would want to know how much we're paying for each square foot. This sounds so simple, but feature engineering in this case involves just dividing sales price by square feet to identify the cheapest house.

```{r cheapest}

ames_fe <- ames |> 
  mutate(cost_sqft = sale_price/sq_ft)

ames_fe |> 
  arrange(cost_sqft) |> 
  head(10) |> 
  kbl() |> 
  kable_styling() |> 
  row_spec(1, bold = T, color = "black", background = "yellow")

```

```{r}
#| label: cost_sqft plot

theme_set(theme_classic())
ggplot(ames_fe, aes(x=cost_sqft)) +
  geom_histogram(fill = "#2A9D8F") + 
  scale_x_continuous(labels=scales::dollar_format()) +
  labs(
    title = "Ames Housing Sale Price",
    x = "Cost per Square Foot",
    y = "Count"
  ) +
  theme(
     plot.title = element_text(hjust = 0.5)
  )

```

Based on our transformation, our distribution is almost perfectly normal.

```{r ames2, include = FALSE}

csf <- ames_fe |> 
  select(id, cost_sqft)

ames2 <- ames_raw |> 
  left_join(csf, by = c("Id" = "id")) |> 
  clean_names() |> 
  select(-c(sale_price, garage_yr_blt)) |> 
  select_if(is.numeric) |> 
  mutate(across(everything(), ~replace_na(.,0)))

```

# Random Forest

In terms of ML feature learning is all about more accurately representing the relationship between your features.The [kaggle tutorial](https://www.kaggle.com/code/ryanholbrook/what-is-feature-engineering) on feature engineering has been really helpful understanding this through code.

```{python ml}
#| echo: true
#| label: ml
#| warning: false
#| error: false

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

ames = r.ames2
X = ames.iloc[:,:-1]
y = ames.iloc[:,-1]

# Train and score baseline model
baseline = RandomForestRegressor(criterion="absolute_error", random_state=0)
baseline_score = cross_val_score(
    baseline, X, y, cv=5, scoring="neg_mean_absolute_error",
    error_score='raise'
)
baseline_score = -1 * baseline_score.mean()

print(f"MAE Baseline Score: {baseline_score:.4}")
```

Looking at just the numeric variables, the MAE is not that bad. There are several variables that could be adjusted, added, or transformed to make them more useful. For example, there are variables for year built and year remodeled. By itself, year remodeled is odd in that new homes have year built as their remodeled year, and some older homes have decades between when they were built and when they were remodeled. Therefore, it might be better just to use the difference between year built and remodeled instead of just the remodeled year. Additionally, there are two variables for bedroom and total rooms, so we know they are already going to be highly correlated with each other. It might be more meaningful from a cost perspective to show the ratio of total rooms to bedrooms to avoid any duplication

```{python}
#| label: first transform


#Difference between yrbuilt and remodeled
X['dif_built_re'] = X['year_built'] - X['year_remod_add']

# Ratio of bedrooms to total rooms
X['bed2room_ratio'] = X['bedroom_abv_gr']/X['tot_rms_abv_grd']

# Train and score updated baseline model
baseline = RandomForestRegressor(criterion="absolute_error", random_state=0)
baseline_score = cross_val_score(
    baseline, X, y, cv=5, scoring="neg_mean_absolute_error",
    error_score='raise'
)
baseline_score = -1 * baseline_score.mean()

print(f"MAE Updated Baseline Score: {baseline_score:.4}")
```

Creating these new variables only slightly improved the model. Oh well!

# Mutual Information

One of the common ways to gain an initial understanding of your data is to measure how different variables are correlated with each other, or rather how two variables are linearly related. This can be done with a correlation matrix of your dataset fairly easily, but what do you do when a non-linear relationship exist? This is where mutual information comes in.

**Mutual Information** is a measure that tells us how much one random variable tells us about another. \[\^1\]

\[\^1\]: Latham, P. E. (2009, January 21). *Mutual information - Scholarpedia*. Scholarpedia. Retrieved May 22, 2022, from http://www.scholarpedia.org/article/Mutual_information\]
