[
  {
    "objectID": "posts/SHAP/index.html",
    "href": "posts/SHAP/index.html",
    "title": "Shap values and model interpretation",
    "section": "",
    "text": "#bibliography: references.bib"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "My Junior Data Scientist Maya"
  },
  {
    "objectID": "posts/SVD/index.html",
    "href": "posts/SVD/index.html",
    "title": "SVD",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/feature_engineering/index.html",
    "href": "posts/feature_engineering/index.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Feature Engineering is the process of selecting, manipulating, and transforming your data into features, or variables, that are useful for machine learning. Whereas data cleaning is generally the process of subtracting irrelevant data, feature engineering is a process of addition, adding more relevant information to your data set.\n\nSimplification\nSimplification is a major part of feature engineering. Combining two or more variables can increase the speed and accuracy of our model. Just to conceptualize what that would look like, I’ve pulled data from the Ames Housing data set on Kaggle. It includes a variable for square footage and final sales prices.\n\n\n\n\n \n  \n    id \n    sq_ft \n    sale_price \n  \n \n\n  \n    1 \n    1710 \n    208500 \n  \n  \n    2 \n    1262 \n    181500 \n  \n  \n    3 \n    1786 \n    223500 \n  \n  \n    4 \n    1717 \n    140000 \n  \n  \n    5 \n    2198 \n    250000 \n  \n  \n    6 \n    1362 \n    143000 \n  \n  \n    7 \n    1694 \n    307000 \n  \n  \n    8 \n    2090 \n    200000 \n  \n  \n    9 \n    1774 \n    129900 \n  \n  \n    10 \n    1077 \n    118000 \n  \n\n\n\n\n\n\n\n\n\n\nIf we were in the neighborhood looking to buy a house, it would not be prudent to only consider the listing price. We would want to know how much we’re paying for each square foot. This sounds so simple, but feature engineering in this case involves just dividing sales price by square feet to identify the cheapest house.\n\n\n\n\n \n  \n    id \n    sq_ft \n    sale_price \n    cost_sqft \n  \n \n\n  \n    1299 \n    5642 \n    160000 \n    28.35874 \n  \n  \n    31 \n    1317 \n    40000 \n    30.37206 \n  \n  \n    1063 \n    2337 \n    90000 \n    38.51091 \n  \n  \n    969 \n    968 \n    37900 \n    39.15289 \n  \n  \n    524 \n    4676 \n    184750 \n    39.51027 \n  \n  \n    1293 \n    2372 \n    107500 \n    45.32040 \n  \n  \n    199 \n    2229 \n    104000 \n    46.65769 \n  \n  \n    411 \n    1276 \n    60000 \n    47.02194 \n  \n  \n    496 \n    720 \n    34900 \n    48.47222 \n  \n  \n    677 \n    1774 \n    87000 \n    49.04171 \n  \n  \n    810 \n    2138 \n    106000 \n    49.57905 \n  \n  \n    706 \n    1092 \n    55000 \n    50.36630 \n  \n  \n    1350 \n    2358 \n    122000 \n    51.73876 \n  \n  \n    884 \n    2230 \n    118500 \n    53.13901 \n  \n  \n    1133 \n    2210 \n    117500 \n    53.16742 \n  \n  \n    1417 \n    2290 \n    122500 \n    53.49345 \n  \n  \n    813 \n    1044 \n    55993 \n    53.63314 \n  \n  \n    1388 \n    2526 \n    136000 \n    53.84006 \n  \n  \n    667 \n    2380 \n    129000 \n    54.20168 \n  \n  \n    243 \n    1440 \n    79000 \n    54.86111 \n  \n\n\n\n\n\n\n\n\n\n\nBased on our transformation, our distribution is almost perfectly normal.\n\n\nRandom Forest\nIn terms of ML feature learning is all about more accurately representing the relationship between your features.The kaggle tutorial on feature engineering has been really helpful understanding this through code.\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\names = r.ames2\nX = ames.iloc[:,:-1]\ny = ames.iloc[:,-1]\n\n# Train and score baseline model\nbaseline = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nbaseline_score = cross_val_score(\n    baseline, X, y, cv=5, scoring=\"neg_mean_absolute_error\",\n    error_score='raise'\n)\nbaseline_score = -1 * baseline_score.mean()\n\nprint(f\"MAE Baseline Score: {baseline_score:.4}\")\n\nMAE Baseline Score: 11.4\n\n\nLooking at just the numeric variables, the MAE is not that bad. There are several variables that could be adjusted, added, or transformed to make them more useful. For example, there are variables for year built and year remodeled. By itself, year remodeled is odd in that new homes have year built as their remodeled year, and some older homes have decades between when they were built and when they were remodeled. Therefore, it might be better just to use the difference between year built and remodeled instead of just the remodeled year. Additionally, there are two variables for bedroom and total rooms, so we know they are already going to be highly correlated with each other. It might be more meaningful from a cost perspective to show the ratio of total rooms to bedrooms to avoid any duplication\n\n\nMAE Updated Baseline Score: 11.38\n\n\nCreating these new variables only slightly improved the model. Oh well!\n\n\nMutual Information\nOne of the common ways to gain an initial understanding of your data is to measure how different variables are correlated with each other, or rather how two variables are linearly related. This can be done with a correlation matrix of your dataset fairly easily, but what do you do when a non-linear relationship exist? This is where mutual information comes in.\nMutual Information is a measure that tells us how much one random variable tells us about another. [^1]\n[^1]: Latham, P. E. (2009, January 21). Mutual information - Scholarpedia. Scholarpedia. Retrieved May 22, 2022, from http://www.scholarpedia.org/article/Mutual_information]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tensor Tenison",
    "section": "",
    "text": "Ethan Tenison\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\nModel Interpretation\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2022\n\n\nEthan Tenison\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfeature engineering\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2022\n\n\nEthan Tenison\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2022\n\n\nEthan Tenison\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Ethan",
    "section": "",
    "text": "Ethan Tenison is a data scientist who is currently working with Sunthetics to bring machine learning to the chemical industry."
  }
]