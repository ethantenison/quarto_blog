[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tensor Tenison",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nFeature Engineering\n\n\n\n\n\n\n\nfeature engineering\n\n\nanalysis\n\n\n\n\n```{r setup, include = FALSE} library(tidyverse) library(scales) library(janitor) library(kableExtra) library(reticulate)\n\n\n\n\n\n\nApr 26, 2022\n\n\nEthan Tenison\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\nData science is my passion. To me it is a super power! But I know in order to be the best data scientist I can be, I need to document my progress and display my work to solidify my learning and be open to feedback.\n\n\n\n\n\n\nApr 23, 2022\n\n\nEthan Tenison\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Ethan",
    "section": "",
    "text": "Ethan Tenison is a data scientist who is currently working with Sunthetics to bring machine learning to the chemical industry."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "My Junior Data Scientist Maya"
  },
  {
    "objectID": "posts/feature_engineering/index.html",
    "href": "posts/feature_engineering/index.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Improving ML models after they’re already been built is fun and can lead to some immediate gratification. You might tweak the hyper parameters and cross validation to improve the generalizability of your model. You may switch out the algorithm itself. Lets say you move from using neural nets to gradient boosted trees to see if you can move the accuracy score even close to 99%. At a certain point though you won’t be able to improve your model without over fitting. Often times, you need to take a step back and look at the raw data itself to see if new variables can be created from the old that lead to improvements. This is what’s called feature engineering\nFeature Engineering is the process of selecting, manipulating, and transforming your data into features, or variables, that are useful for machine learning. Whereas data cleaning is generally the process of subtracting irrelevant data, feature engineering is a process of addition, adding more relevant information to your data set.\n\nSimplification\nSimplification is a major part of feature engineering. Combining two or more variables can increase the speed and accuracy of our model. Just to conceptualize what that would look like, I’ve pulled data from the Ames Housing data set on Kaggle. It includes a variable for square footage and final sales prices.\n\n\n\n \n  \n    id \n    sq_ft \n    sale_price \n  \n \n\n  \n    1 \n    1710 \n    208500 \n  \n  \n    2 \n    1262 \n    181500 \n  \n  \n    3 \n    1786 \n    223500 \n  \n  \n    4 \n    1717 \n    140000 \n  \n  \n    5 \n    2198 \n    250000 \n  \n  \n    6 \n    1362 \n    143000 \n  \n  \n    7 \n    1694 \n    307000 \n  \n  \n    8 \n    2090 \n    200000 \n  \n  \n    9 \n    1774 \n    129900 \n  \n  \n    10 \n    1077 \n    118000 \n  \n\n\n\n\n\n\n\n\n\n\nIf we were in the neighborhood looking to buy a house, it would not be prudent to only consider the listing price. We would want to know how much we’re paying for each square foot. This sounds so simple, but feature engineering in this case involves just dividing sales price by square feet to identify the cheapest house.\n\n\n\n \n  \n    id \n    sq_ft \n    sale_price \n    cost_sqft \n  \n \n\n  \n    1299 \n    5642 \n    160000 \n    28.35874 \n  \n  \n    31 \n    1317 \n    40000 \n    30.37206 \n  \n  \n    1063 \n    2337 \n    90000 \n    38.51091 \n  \n  \n    969 \n    968 \n    37900 \n    39.15289 \n  \n  \n    524 \n    4676 \n    184750 \n    39.51027 \n  \n  \n    1293 \n    2372 \n    107500 \n    45.32040 \n  \n  \n    199 \n    2229 \n    104000 \n    46.65769 \n  \n  \n    411 \n    1276 \n    60000 \n    47.02194 \n  \n  \n    496 \n    720 \n    34900 \n    48.47222 \n  \n  \n    677 \n    1774 \n    87000 \n    49.04171 \n  \n  \n    810 \n    2138 \n    106000 \n    49.57905 \n  \n  \n    706 \n    1092 \n    55000 \n    50.36630 \n  \n  \n    1350 \n    2358 \n    122000 \n    51.73876 \n  \n  \n    884 \n    2230 \n    118500 \n    53.13901 \n  \n  \n    1133 \n    2210 \n    117500 \n    53.16742 \n  \n  \n    1417 \n    2290 \n    122500 \n    53.49345 \n  \n  \n    813 \n    1044 \n    55993 \n    53.63314 \n  \n  \n    1388 \n    2526 \n    136000 \n    53.84006 \n  \n  \n    667 \n    2380 \n    129000 \n    54.20168 \n  \n  \n    243 \n    1440 \n    79000 \n    54.86111 \n  \n\n\n\n\n\n\n\n\n\n\nBased on our transformation, our distribution is almost perfectly normal.\n\n\nRandom Forest\nIn terms of ML feature learning is all about more accurately representing the relationship between your features.The kaggle tutorial on feature engineering has been really helpful understanding this through code.\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\names = r.ames2\nX = ames.iloc[:,:-1]\ny = ames.iloc[:,-1]\n\n# Train and score baseline model\nbaseline = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nbaseline_score = cross_val_score(\n    baseline, X, y, cv=5, scoring=\"neg_mean_absolute_error\",\n    error_score='raise'\n)\nbaseline_score = -1 * baseline_score.mean()\n\nprint(f\"MAE Baseline Score: {baseline_score:.4}\")\n\nMAE Baseline Score: 11.4\n\n\nLooking at just the numeric variables, the MAE is not that bad. There are several variables that could be adjusted, added, or transformed to make them more useful. For example, there are variables for year built and year remodeled. By itself, year remodeled is odd in that new homes have year built as their remodeled year, and some older homes have decades between when they were built and when they were remodeled. Therefore, it might be better just to use the difference between year built and remodeled instead of just the remodeled year. Additionally, there are two variables for bedroom and total rooms, so we know they are already going to be highly correlated with each other. It might be more meaningful from a cost perspective to show the ratio of total rooms to bedrooms to avoid any duplication\n\n\nMAE Updated Baseline Score: 11.38\n\n\nCreating these new variables only slightly improved the model. Oh well!\n\n\nMutual Information\nOne of the common ways to gain an initial understanding of your data is to measure how different variables are correlated with each other, or rather how two variables are linearly related. This can be done with a correlation matrix of your dataset fairly easily, but what do you do when a non-linear relationship exist? This is where mutual information comes in.\nMutual Information is a measure that tells us how much one random variable tells us about another. [^1]\n[^1]: Latham, P. E. (2009, January 21). Mutual information - Scholarpedia. Scholarpedia. Retrieved May 22, 2022, from http://www.scholarpedia.org/article/Mutual_information]"
  }
]